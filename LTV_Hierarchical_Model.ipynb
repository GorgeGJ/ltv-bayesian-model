{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Lifetime Value Modeling with PyMC: Hierarchical BG/NBD, Gamma-Gamma, and Cohort Decay\n",
    "\n",
    "# ðŸ“¦ Dependencies\n",
    "!pip install pymc pymc-marketing pandas matplotlib seaborn numpy\n",
    "\n",
    "# ðŸ“š Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# ðŸŽ² Step 1: Simulate Sample Transaction Data\n",
    "np.random.seed(42)\n",
    "n_customers = 500\n",
    "channels = ['Organic', 'Paid', 'Referral']\n",
    "\n",
    "customer_df = pd.DataFrame({\n",
    "    'customer_id': [f'C{i}' for i in range(n_customers)],\n",
    "    'channel': np.random.choice(channels, size=n_customers, p=[0.4, 0.4, 0.2]),\n",
    "})\n",
    "\n",
    "customer_df['acquisition_date'] = pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 30, n_customers), unit='D')\n",
    "\n",
    "transactions = []\n",
    "for _, row in customer_df.iterrows():\n",
    "    cust_id = row['customer_id']\n",
    "    channel = row['channel']\n",
    "    start_date = row['acquisition_date']\n",
    "    num_txn = np.random.poisson(3 if channel == 'Organic' else 2 if channel == 'Referral' else 1.5)\n",
    "    churn_prob = 0.2 if channel == 'Organic' else 0.35 if channel == 'Referral' else 0.5\n",
    "    order_value_mean = 60 if channel == 'Organic' else 90 if channel == 'Paid' else 70\n",
    "\n",
    "    date = start_date\n",
    "    for _ in range(num_txn):\n",
    "        if np.random.rand() < churn_prob:\n",
    "            break\n",
    "        amount = np.random.gamma(order_value_mean / 10, 10)\n",
    "        transactions.append([cust_id, date, amount, channel])\n",
    "        date += pd.to_timedelta(np.random.randint(7, 60), unit='D')\n",
    "        if date > pd.to_datetime('2023-06-30'):\n",
    "            break\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions, columns=['customer_id', 'purchase_date', 'revenue', 'channel'])\n",
    "transactions_df.to_csv(\"sample_transactions.csv\", index=False)\n",
    "\n",
    "# ðŸ“Š Step 2: Prepare Summary for Hierarchical Modeling\n",
    "transactions_df = pd.read_csv(\"sample_transactions.csv\")\n",
    "transactions_df['purchase_date'] = pd.to_datetime(transactions_df['purchase_date'])\n",
    "\n",
    "# Assign cohort index\n",
    "grouped_channels, cohort_codes = pd.factorize(transactions_df['channel'])\n",
    "transactions_df['channel_code'] = grouped_channels\n",
    "\n",
    "from pymc_marketing.utils import rfm_summary\n",
    "\n",
    "summary = rfm_summary(\n",
    "    transactions_df,\n",
    "    customer_id_col=\"customer_id\",\n",
    "    datetime_col=\"purchase_date\",\n",
    "    monetary_value_col=\"revenue\",\n",
    "    observation_period_end=pd.to_datetime(\"2023-07-01\")\n",
    ")\n",
    "\n",
    "customer_channels = transactions_df.drop_duplicates(subset='customer_id')[['customer_id', 'channel', 'channel_code']]\n",
    "summary = summary.merge(customer_channels, left_index=True, right_on='customer_id')\n",
    "\n",
    "# ðŸ§  Step 3: Hierarchical BG/NBD + Gamma-Gamma Model with Custom Priors\n",
    "with pm.Model() as hierarchical_ltv_model:\n",
    "    n_cohorts = len(cohort_codes)\n",
    "    cohort_idx = summary['channel_code'].values\n",
    "\n",
    "    mu_r = pm.Gamma(\"mu_r\", alpha=2, beta=0.5)\n",
    "    sigma_r = pm.HalfCauchy(\"sigma_r\", beta=1)\n",
    "    mu_alpha = pm.Gamma(\"mu_alpha\", alpha=2, beta=0.5)\n",
    "    sigma_alpha = pm.HalfCauchy(\"sigma_alpha\", beta=1)\n",
    "    mu_a = pm.Gamma(\"mu_a\", alpha=2, beta=0.5)\n",
    "    sigma_a = pm.HalfCauchy(\"sigma_a\", beta=1)\n",
    "    mu_b = pm.Gamma(\"mu_b\", alpha=2, beta=0.5)\n",
    "    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=1)\n",
    "\n",
    "    r = pm.LogNormal(\"r\", mu=mu_r, sigma=sigma_r, shape=n_cohorts)\n",
    "    alpha = pm.LogNormal(\"alpha\", mu=mu_alpha, sigma=sigma_alpha, shape=n_cohorts)\n",
    "    a = pm.LogNormal(\"a\", mu=mu_a, sigma=sigma_a, shape=n_cohorts)\n",
    "    b = pm.LogNormal(\"b\", mu=mu_b, sigma=sigma_b, shape=n_cohorts)\n",
    "\n",
    "    freq = summary['frequency'].values\n",
    "    rec = summary['recency'].values\n",
    "    T = summary['T'].values\n",
    "\n",
    "    ll_bgnbd = (\n",
    "        pm.math.gammaln(r[cohort_idx] + freq)\n",
    "        - pm.math.gammaln(r[cohort_idx])\n",
    "        + r[cohort_idx] * pm.math.log(alpha[cohort_idx])\n",
    "        + freq * pm.math.log(rec + alpha[cohort_idx])\n",
    "        - (r[cohort_idx] + freq) * pm.math.log(T + alpha[cohort_idx])\n",
    "    )\n",
    "    pm.Potential(\"bgnbd_likelihood\", ll_bgnbd.sum())\n",
    "\n",
    "    mv = summary['monetary_value'].values\n",
    "    p = pm.Gamma(\"p\", alpha=2, beta=0.5)\n",
    "    q = pm.Gamma(\"q\", alpha=2, beta=0.5)\n",
    "    v = pm.Gamma(\"v\", alpha=2, beta=0.5)\n",
    "\n",
    "    pm.Gamma(\"monetary_obs\", alpha=p, beta=v / (freq + q), observed=mv)\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, target_accept=0.9, return_inferencedata=True)\n",
    "\n",
    "# ðŸ“ˆ Posterior Diagnostics\n",
    "az.plot_trace(trace)\n",
    "plt.suptitle(\"Hierarchical Model Posterior Trace\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "az.plot_posterior(trace)\n",
    "plt.suptitle(\"Posterior Distributions by Cohort\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“‹ Inference Summary\n",
    "inference_summary = az.summary(trace, round_to=2)\n",
    "print(\"\\nCustom Inference Report:\\n\")\n",
    "print(inference_summary[['mean', 'hdi_3%', 'hdi_97%', 'ess_bulk', 'r_hat']])\n",
    "\n",
    "# ðŸ“‰ Empirical Cohort Decay Curves\n",
    "transactions_df['week'] = transactions_df['purchase_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "cohort_retention = transactions_df.groupby(['channel', 'week'])['customer_id'].nunique().reset_index()\n",
    "cohort_retention['week_num'] = cohort_retention.groupby('channel')['week'].rank().astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for channel in channels:\n",
    "    temp = cohort_retention[cohort_retention['channel'] == channel]\n",
    "    base = temp['customer_id'].iloc[0]\n",
    "    plt.plot(temp['week_num'], temp['customer_id'] / base, label=channel)\n",
    "\n",
    "plt.title('Empirical Cohort Decay Curves by Channel')\n",
    "plt.xlabel('Week since acquisition')\n",
    "plt.ylabel('% of active customers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}